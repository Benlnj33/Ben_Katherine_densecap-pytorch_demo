{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "A100",
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# Check if you are using a gpu runtime in collab\n",
        "\n",
        "import torch\n",
        "\n",
        "# Check if CUDA (GPU) is available\n",
        "if torch.cuda.is_available():\n",
        "    print(\"CUDA is available. GPU is accessible.\")\n",
        "    print(f\"Number of available GPUs: {torch.cuda.device_count()}\")\n",
        "    print(f\"Name of current GPU: {torch.cuda.get_device_name(0)}\")\n",
        "else:\n",
        "    print(\"CUDA is not available. Using CPU instead.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z8OVswZGbuwm",
        "outputId": "62a07fd0-d512-4eed-b203-86bf47f57e62"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CUDA is available. GPU is accessible.\n",
            "Number of available GPUs: 1\n",
            "Name of current GPU: NVIDIA A100-SXM4-40GB\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "YFNnV-rQz8GZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e49050f6-69e6-464c-a325-fa954c37fed8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'densecap-pytorch'...\n",
            "remote: Enumerating objects: 90, done.\u001b[K\n",
            "remote: Counting objects: 100% (40/40), done.\u001b[K\n",
            "remote: Compressing objects: 100% (13/13), done.\u001b[K\n",
            "remote: Total 90 (delta 29), reused 27 (delta 27), pack-reused 50\u001b[K\n",
            "Receiving objects: 100% (90/90), 5.23 MiB | 3.83 MiB/s, done.\n",
            "Resolving deltas: 100% (39/39), done.\n"
          ]
        }
      ],
      "source": [
        "#clone densecap-pytorch demo\n",
        "\n",
        "!git clone https://github.com/soloist97/densecap-pytorch.git"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install torch torchvision\n",
        "!pip install apex"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3Xb4ZT8ihX4n",
        "outputId": "3a1509cc-bc24-419b-8ad4-0c61a7287435"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.3.0)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (0.18.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch) (3.14.0)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch) (4.11.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.3)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch) (2024.3.1)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /usr/local/lib/python3.10/dist-packages (from torch) (8.9.2.26)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /usr/local/lib/python3.10/dist-packages (from torch) (12.1.3.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /usr/local/lib/python3.10/dist-packages (from torch) (11.0.2.54)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /usr/local/lib/python3.10/dist-packages (from torch) (10.3.2.106)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /usr/local/lib/python3.10/dist-packages (from torch) (11.4.5.107)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /usr/local/lib/python3.10/dist-packages (from torch) (12.1.0.106)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.20.5 in /usr/local/lib/python3.10/dist-packages (from torch) (2.20.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch) (12.1.105)\n",
            "Requirement already satisfied: triton==2.3.0 in /usr/local/lib/python3.10/dist-packages (from torch) (2.3.0)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.10/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch) (12.4.127)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torchvision) (1.25.2)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from torchvision) (9.4.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (2.1.5)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch) (1.3.0)\n",
            "Collecting apex\n",
            "  Downloading apex-0.9.10dev.tar.gz (36 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting cryptacular (from apex)\n",
            "  Downloading cryptacular-1.6.2.tar.gz (75 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m75.8/75.8 kB\u001b[0m \u001b[31m4.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting zope.sqlalchemy (from apex)\n",
            "  Downloading zope.sqlalchemy-3.1-py3-none-any.whl (23 kB)\n",
            "Collecting velruse>=1.0.3 (from apex)\n",
            "  Downloading velruse-1.1.1.tar.gz (709 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m709.8/709.8 kB\u001b[0m \u001b[31m30.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting pyramid>1.1.2 (from apex)\n",
            "  Downloading pyramid-2.0.2-py3-none-any.whl (247 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m247.3/247.3 kB\u001b[0m \u001b[31m32.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting pyramid_mailer (from apex)\n",
            "  Downloading pyramid_mailer-0.15.1-py2.py3-none-any.whl (19 kB)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from apex) (2.31.0)\n",
            "Collecting wtforms (from apex)\n",
            "  Downloading wtforms-3.1.2-py3-none-any.whl (145 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m146.0/146.0 kB\u001b[0m \u001b[31m21.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting wtforms-recaptcha (from apex)\n",
            "  Downloading wtforms_recaptcha-0.3.2-py2.py3-none-any.whl (7.5 kB)\n",
            "Collecting hupper>=1.5 (from pyramid>1.1.2->apex)\n",
            "  Downloading hupper-1.12.1-py3-none-any.whl (22 kB)\n",
            "Collecting plaster (from pyramid>1.1.2->apex)\n",
            "  Downloading plaster-1.1.2-py2.py3-none-any.whl (11 kB)\n",
            "Collecting plaster-pastedeploy (from pyramid>1.1.2->apex)\n",
            "  Downloading plaster_pastedeploy-1.0.1-py2.py3-none-any.whl (7.8 kB)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from pyramid>1.1.2->apex) (67.7.2)\n",
            "Collecting translationstring>=0.4 (from pyramid>1.1.2->apex)\n",
            "  Downloading translationstring-1.4-py2.py3-none-any.whl (15 kB)\n",
            "Collecting venusian>=1.0 (from pyramid>1.1.2->apex)\n",
            "  Downloading venusian-3.1.0-py3-none-any.whl (13 kB)\n",
            "Collecting webob>=1.8.3 (from pyramid>1.1.2->apex)\n",
            "  Downloading WebOb-1.8.7-py2.py3-none-any.whl (114 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m115.0/115.0 kB\u001b[0m \u001b[31m17.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting zope.deprecation>=3.5.0 (from pyramid>1.1.2->apex)\n",
            "  Downloading zope.deprecation-5.0-py3-none-any.whl (10 kB)\n",
            "Collecting zope.interface>=3.8.0 (from pyramid>1.1.2->apex)\n",
            "  Downloading zope.interface-6.3-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (247 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m247.3/247.3 kB\u001b[0m \u001b[31m30.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: requests-oauthlib in /usr/local/lib/python3.10/dist-packages (from velruse>=1.0.3->apex) (1.3.1)\n",
            "Collecting anykeystore (from velruse>=1.0.3->apex)\n",
            "  Downloading anykeystore-0.2.tar.gz (10 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting python3-openid (from velruse>=1.0.3->apex)\n",
            "  Downloading python3_openid-3.2.0-py3-none-any.whl (133 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m133.7/133.7 kB\u001b[0m \u001b[31m20.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting pbkdf2 (from cryptacular->apex)\n",
            "  Downloading pbkdf2-1.3.tar.gz (6.4 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting repoze.sendmail>=4.1 (from pyramid_mailer->apex)\n",
            "  Downloading repoze.sendmail-4.4.1-py2.py3-none-any.whl (41 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m41.0/41.0 kB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting transaction (from pyramid_mailer->apex)\n",
            "  Downloading transaction-4.0-py3-none-any.whl (46 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.6/46.6 kB\u001b[0m \u001b[31m6.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->apex) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->apex) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->apex) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->apex) (2024.2.2)\n",
            "Requirement already satisfied: markupsafe in /usr/local/lib/python3.10/dist-packages (from wtforms->apex) (2.1.5)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from zope.sqlalchemy->apex) (24.0)\n",
            "Requirement already satisfied: SQLAlchemy!=1.4.0,!=1.4.1,!=1.4.2,!=1.4.3,!=1.4.4,!=1.4.5,!=1.4.6,>=1.1 in /usr/local/lib/python3.10/dist-packages (from zope.sqlalchemy->apex) (2.0.29)\n",
            "Requirement already satisfied: typing-extensions>=4.6.0 in /usr/local/lib/python3.10/dist-packages (from SQLAlchemy!=1.4.0,!=1.4.1,!=1.4.2,!=1.4.3,!=1.4.4,!=1.4.5,!=1.4.6,>=1.1->zope.sqlalchemy->apex) (4.11.0)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from SQLAlchemy!=1.4.0,!=1.4.1,!=1.4.2,!=1.4.3,!=1.4.4,!=1.4.5,!=1.4.6,>=1.1->zope.sqlalchemy->apex) (3.0.3)\n",
            "Collecting PasteDeploy>=2.0 (from plaster-pastedeploy->pyramid>1.1.2->apex)\n",
            "  Downloading PasteDeploy-3.1.0-py3-none-any.whl (16 kB)\n",
            "Requirement already satisfied: defusedxml in /usr/local/lib/python3.10/dist-packages (from python3-openid->velruse>=1.0.3->apex) (0.7.1)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from requests-oauthlib->velruse>=1.0.3->apex) (3.2.2)\n",
            "Building wheels for collected packages: apex, velruse, cryptacular, anykeystore, pbkdf2\n",
            "  Building wheel for apex (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for apex: filename=apex-0.9.10.dev0-py3-none-any.whl size=46442 sha256=0b5df5ea44cf5e4d2766aa71b3bd8ccf7d9262a726e6b5de7a2d38ee09b7c23b\n",
            "  Stored in directory: /root/.cache/pip/wheels/6e/62/59/9b100fce7ebd989603b3b7a4ca259150da72c9e107fcaa2a30\n",
            "  Building wheel for velruse (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for velruse: filename=velruse-1.1.1-py3-none-any.whl size=50909 sha256=2f74afc37fdff78008d36359b3e7c5dcca4320980d9fb6b5a3b2a5f377f179a0\n",
            "  Stored in directory: /root/.cache/pip/wheels/4a/f9/a4/fc4ea7b935ee9c58b9bc772cabd94f6a8560f35444097d948d\n",
            "  Building wheel for cryptacular (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for cryptacular: filename=cryptacular-1.6.2-cp310-cp310-linux_x86_64.whl size=55086 sha256=3456d58781ec1696ad2e6f843a8126a567e28d548bd9831c67ad0b8ab7535827\n",
            "  Stored in directory: /root/.cache/pip/wheels/3f/6e/09/a7fba517f95b2a6a36bd01b6d4f4679fa7259615a493b64b8f\n",
            "  Building wheel for anykeystore (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for anykeystore: filename=anykeystore-0.2-py3-none-any.whl size=16813 sha256=0857a7a37488038538d21fd1a33bd2eb690268122b7d8604cefa55ee199a4f0d\n",
            "  Stored in directory: /root/.cache/pip/wheels/ce/9e/24/35542b7d376b53a6f8426524cc5a3f7998f975037b32d19906\n",
            "  Building wheel for pbkdf2 (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pbkdf2: filename=pbkdf2-1.3-py3-none-any.whl size=5083 sha256=cf4e01e5e3434be61c01ca2900f6550111526258ebcaffdb81e3f933c4c074c0\n",
            "  Stored in directory: /root/.cache/pip/wheels/f6/7d/8b/4269ff90fda80497ec59f6ff7d1e1596cb697c1dc8e9bbe320\n",
            "Successfully built apex velruse cryptacular anykeystore pbkdf2\n",
            "Installing collected packages: translationstring, pbkdf2, anykeystore, zope.interface, zope.deprecation, wtforms, webob, venusian, python3-openid, plaster, PasteDeploy, hupper, cryptacular, wtforms-recaptcha, transaction, plaster-pastedeploy, zope.sqlalchemy, repoze.sendmail, pyramid, velruse, pyramid_mailer, apex\n",
            "Successfully installed PasteDeploy-3.1.0 anykeystore-0.2 apex-0.9.10.dev0 cryptacular-1.6.2 hupper-1.12.1 pbkdf2-1.3 plaster-1.1.2 plaster-pastedeploy-1.0.1 pyramid-2.0.2 pyramid_mailer-0.15.1 python3-openid-3.2.0 repoze.sendmail-4.4.1 transaction-4.0 translationstring-1.4 velruse-1.1.1 venusian-3.1.0 webob-1.8.7 wtforms-3.1.2 wtforms-recaptcha-0.3.2 zope.deprecation-5.0 zope.interface-6.3 zope.sqlalchemy-3.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "import zipfile\n",
        "import io\n",
        "import os\n",
        "\n",
        "parent_directory = \"densecap-pytorch\"\n",
        "data_directory = \"data\"\n",
        "subdirectories = [\"visual-genome\"]\n",
        "data_path = os.path.join(parent_directory, data_directory)\n",
        "\n",
        "if not os.path.exists(parent_directory):\n",
        "    os.makedirs(parent_directory)\n",
        "    print(f\"Parent directory '{parent_directory}' created successfully.\")\n",
        "else:\n",
        "    print(f\"Parent directory '{parent_directory}' already exists.\")\n",
        "\n",
        "if not os.path.exists(data_path):\n",
        "    os.makedirs(data_path)\n",
        "    print(f\"Data directory '{data_directory}' created successfully.\")\n",
        "else:\n",
        "    print(f\"Data directory '{data_directory}' already exists.\")\n",
        "\n",
        "for subdir in subdirectories:\n",
        "    subdir_path = os.path.join(data_path, subdir)\n",
        "    if not os.path.exists(subdir_path):\n",
        "        os.makedirs(subdir_path)\n",
        "        print(f\"Subdirectory '{subdir}' created successfully.\")\n",
        "    else:\n",
        "        print(f\"Subdirectory '{subdir}' already exists.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GyB1qIfI_oYV",
        "outputId": "683b7c01-08f0-4c16-e405-af4dad8e8f53"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Parent directory 'densecap-pytorch' already exists.\n",
            "Data directory 'data' already exists.\n",
            "Subdirectory 'model_params' already exists.\n",
            "Subdirectory 'visual-genome' already exists.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#download image data (18 MB)\n",
        "zip_url = \"https://homes.cs.washington.edu/~ranjay/visualgenome/data/dataset/image_data.json.zip\"\n",
        "extract_to_directory = \"densecap-pytorch/data/visual-genome\"\n",
        "response = requests.get(zip_url)\n",
        "\n",
        "if response.status_code == 200:\n",
        "    zip_content = io.BytesIO(response.content)\n",
        "\n",
        "    with zipfile.ZipFile(zip_content, 'r') as zip_ref:\n",
        "        zip_ref.extractall(extract_to_directory)\n",
        "\n",
        "    print(\"Extraction completed successfully.\")\n",
        "else:\n",
        "    print(\"Failed to download the zip file.\")\n",
        "\n",
        "#frees up ram in collab\n",
        "del response, zip_content"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6shnEvZXA7l8",
        "outputId": "3aaa59dd-fdb7-4251-9f63-404ff2b97fc7"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extraction completed successfully.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# download region descriptions (712 MB)\n",
        "zip_url = \"https://homes.cs.washington.edu/~ranjay/visualgenome/data/dataset/region_descriptions.json.zip\"\n",
        "extract_to_directory = \"densecap-pytorch/data/visual-genome\"\n",
        "response = requests.get(zip_url)\n",
        "\n",
        "if response.status_code == 200:\n",
        "    zip_content = io.BytesIO(response.content)\n",
        "\n",
        "    with zipfile.ZipFile(zip_content, 'r') as zip_ref:\n",
        "        zip_ref.extractall(extract_to_directory)\n",
        "\n",
        "    print(\"Extraction completed successfully.\")\n",
        "else:\n",
        "    print(\"Failed to download the zip file.\")\n",
        "\n",
        "\n",
        "del response, zip_content"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nHGO4alvIZNq",
        "outputId": "c2e7cd46-fd52-40b4-f20c-acba0e3e23aa"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extraction completed successfully.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# download VG_100K (9.2 GB)\n",
        "zip_url = \"https://cs.stanford.edu/people/rak248/VG_100K_2/images.zip\"\n",
        "extract_to_directory = \"densecap-pytorch/data/visual-genome\"\n",
        "response = requests.get(zip_url)\n",
        "\n",
        "if response.status_code == 200:\n",
        "    zip_content = io.BytesIO(response.content)\n",
        "\n",
        "    with zipfile.ZipFile(zip_content, 'r') as zip_ref:\n",
        "        zip_ref.extractall(extract_to_directory)\n",
        "\n",
        "    print(\"Extraction completed successfully.\")\n",
        "else:\n",
        "    print(\"Failed to download the zip file.\")\n",
        "\n",
        "\n",
        "del response, zip_content"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g3ynt14QP0-o",
        "outputId": "819aabc4-f316-4efd-f2e3-c68ed9e315ef"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extraction completed successfully.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# download VG_100K_2 (5.5 GB)\n",
        "zip_url = \"https://cs.stanford.edu/people/rak248/VG_100K_2/images2.zip\"\n",
        "extract_to_directory = \"densecap-pytorch/data/visual-genome\"\n",
        "response = requests.get(zip_url)\n",
        "\n",
        "if response.status_code == 200:\n",
        "    zip_content = io.BytesIO(response.content)\n",
        "\n",
        "    with zipfile.ZipFile(zip_content, 'r') as zip_ref:\n",
        "        zip_ref.extractall(extract_to_directory)\n",
        "\n",
        "    print(\"Extraction completed successfully.\")\n",
        "else:\n",
        "    print(\"Failed to download the zip file.\")\n",
        "\n",
        "del response, zip_content"
      ],
      "metadata": {
        "id": "VbnSN-c5JUEv",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3390f20c-ecbc-40a3-f0c9-20b2eaed55d3"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extraction completed successfully.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#preprocess the data\n",
        "%cd /content/densecap-pytorch\n",
        "\n",
        "!python preprocess.py"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MRYWMHUicPt_",
        "outputId": "21e1765f-49f5-4eb3-f0e7-ae6624027f7e"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/densecap-pytorch\n",
            "There are 108077 images total\n",
            "After filtering for splits there are 87398 images\n",
            "Splitting tokens in image 2000 / 87398\n",
            "Splitting tokens in image 4000 / 87398\n",
            "Splitting tokens in image 6000 / 87398\n",
            "Splitting tokens in image 8000 / 87398\n",
            "Splitting tokens in image 10000 / 87398\n",
            "Splitting tokens in image 12000 / 87398\n",
            "Splitting tokens in image 14000 / 87398\n",
            "Splitting tokens in image 16000 / 87398\n",
            "Splitting tokens in image 18000 / 87398\n",
            "Splitting tokens in image 20000 / 87398\n",
            "Splitting tokens in image 22000 / 87398\n",
            "Splitting tokens in image 24000 / 87398\n",
            "Splitting tokens in image 26000 / 87398\n",
            "Splitting tokens in image 28000 / 87398\n",
            "Splitting tokens in image 30000 / 87398\n",
            "Splitting tokens in image 32000 / 87398\n",
            "Splitting tokens in image 34000 / 87398\n",
            "Splitting tokens in image 36000 / 87398\n",
            "Splitting tokens in image 38000 / 87398\n",
            "Splitting tokens in image 40000 / 87398\n",
            "Splitting tokens in image 42000 / 87398\n",
            "Splitting tokens in image 44000 / 87398\n",
            "Splitting tokens in image 46000 / 87398\n",
            "Splitting tokens in image 48000 / 87398\n",
            "Splitting tokens in image 50000 / 87398\n",
            "Splitting tokens in image 52000 / 87398\n",
            "Splitting tokens in image 54000 / 87398\n",
            "Splitting tokens in image 56000 / 87398\n",
            "Splitting tokens in image 58000 / 87398\n",
            "Splitting tokens in image 60000 / 87398\n",
            "Splitting tokens in image 62000 / 87398\n",
            "Splitting tokens in image 64000 / 87398\n",
            "Splitting tokens in image 66000 / 87398\n",
            "Splitting tokens in image 68000 / 87398\n",
            "Splitting tokens in image 70000 / 87398\n",
            "Splitting tokens in image 72000 / 87398\n",
            "Splitting tokens in image 74000 / 87398\n",
            "Splitting tokens in image 76000 / 87398\n",
            "Splitting tokens in image 78000 / 87398\n",
            "Splitting tokens in image 80000 / 87398\n",
            "Splitting tokens in image 82000 / 87398\n",
            "Splitting tokens in image 84000 / 87398\n",
            "Splitting tokens in image 86000 / 87398\n",
            "Keeping 4158176 captions\n",
            "Skipped 1318 captions for being too long\n",
            "Keeping 10625 / 61817 tokens with enough instances\n",
            "adding special <pad> <bos> <eos> <unk> token.\n",
            "invalid y2 coordinate 769 > 768 in image_id:285665 box_id:5945019\n",
            "orignal bbox coordinate  (372, 471, 623, 769)\n",
            "clamped bbox coordinate  (372, 471, 623, 767)\n",
            "invalid y2 coordinate 1022 > 768 in image_id:285665 box_id:5945020\n",
            "orignal bbox coordinate  (374, 718, 549, 1022)\n",
            "clamped bbox coordinate  (374, 718, 549, 767)\n",
            "invalid y2 coordinate 1023 > 768 in image_id:285665 box_id:5945025\n",
            "orignal bbox coordinate  (226, 361, 717, 1023)\n",
            "clamped bbox coordinate  (226, 361, 717, 767)\n",
            "invalid x2 coordinate 1026 > 1024 in image_id:713163 box_id:5687536\n",
            "orignal bbox coordinate  (0, 2, 1026, 514)\n",
            "clamped bbox coordinate  (0, 2, 1023, 514)\n",
            "invalid y1 coordinate 1016 > 769 in image_id:1159528 box_id:6014110\n",
            "orignal bbox coordinate  (363, 1016, 645, 1346)\n",
            "clamped bbox coordinate  (363, 438, 645, 768)\n",
            "invalid y1 coordinate 933 > 768 in image_id:1159571 box_id:6015405\n",
            "orignal bbox coordinate  (363, 933, 497, 1107)\n",
            "clamped bbox coordinate  (363, 593, 497, 767)\n",
            "invalid y1 coordinate 774 > 646 in image_id:1591814 box_id:6038277\n",
            "orignal bbox coordinate  (376, 774, 497, 930)\n",
            "clamped bbox coordinate  (376, 489, 497, 645)\n",
            "invalid y1 coordinate 885 > 576 in image_id:1593117 box_id:6080243\n",
            "orignal bbox coordinate  (167, 885, 203, 892)\n",
            "clamped bbox coordinate  (167, 568, 203, 575)\n",
            "invalid y1 coordinate 513 > 333 in image_id:2343049 box_id:6091909\n",
            "orignal bbox coordinate  (203, 513, 220, 516)\n",
            "clamped bbox coordinate  (203, 329, 220, 332)\n",
            "invalid y1 coordinate 472 > 208 in image_id:2328539 box_id:5887160\n",
            "orignal bbox coordinate  (223, 472, 301, 486)\n",
            "clamped bbox coordinate  (223, 193, 301, 207)\n",
            "invalid y1 coordinate 539 > 333 in image_id:2327975 box_id:5894475\n",
            "orignal bbox coordinate  (190, 539, 207, 542)\n",
            "clamped bbox coordinate  (190, 329, 207, 332)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#create model_params folder\n",
        "directory = \"/content/densecap-pytorch/model_params\"\n",
        "\n",
        "if not os.path.exists(directory):\n",
        "    os.makedirs(directory)\n",
        "    print(f\"Directory '{directory}' created successfully.\")\n",
        "else:\n",
        "    print(f\"Directory '{directory}' already exists.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qU14YQlnfCF2",
        "outputId": "d310ca65-99b6-4335-c497-9e20a0f2fa0f"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Directory '/content/densecap-pytorch/model_params' created successfully.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#install NVIDIA Apex\n",
        "\n",
        "%cd /content/densecap-pytorch\n",
        "!git clone https://github.com/NVIDIA/apex\n",
        "\n",
        "%cd apex\n",
        "!python setup.py install"
      ],
      "metadata": {
        "id": "OHAdYUg1guvp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#install nlgeval and run script to train a new model (SKIP IF YOU WANT TO USE PRE TRAINED MODEL)\n",
        "\n",
        "%cd /content/densecap-pytorch\n",
        "!pip install git+https://github.com/Maluuba/nlg-eval.git@master\n",
        "\n",
        "%cd /content/densecap-pytorch\n",
        "!python train.py"
      ],
      "metadata": {
        "id": "9DxGvQhUgA7u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#download pretrained model\n",
        "\n",
        "os.chdir(\"/content/densecap-pytorch/model_params\")\n",
        "onedrive_link = \"https://1drv.ms/u/s!AmN4YCVEJTAIhcEL0HQDwgl79REDzQ?e=xYKEO1\"\n",
        "!wget --no-check-certificate -r \"$onedrive_link\"\n",
        "\n",
        "print(\"Download completed successfully.\")"
      ],
      "metadata": {
        "id": "Zze-qFNDPuM6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#train from pretrained model checkpoint\n",
        "\n",
        "%cd /content/densecap-pytorch\n",
        "\n",
        "import os\n",
        "import json\n",
        "\n",
        "import torch\n",
        "import numpy as np\n",
        "from torch.utils.data.dataset import Subset\n",
        "from torchvision.models.detection.faster_rcnn import fasterrcnn_resnet50_fpn\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "\n",
        "from utils.data_loader import TheatreDataset, TheatreDataLoaderPFG\n",
        "from model.densecap import densecap_resnet50_fpn\n",
        "\n",
        "from apex import amp\n",
        "\n",
        "from evaluate import quality_check, quantity_check\n",
        "\n",
        "torch.backends.cudnn.benchmark = True\n",
        "np.random.seed(42)\n",
        "torch.manual_seed(42)\n",
        "torch.cuda.manual_seed(42)\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "PATH = \"/content/densecap-pytorch/model_params/train_all_val_all_bz_2_epoch_10_inject_init_end.pth.tar\"\n",
        "\n",
        "\n",
        "MAX_EPOCHS = 10\n",
        "USE_TB = True\n",
        "CONFIG_PATH = './model_params'\n",
        "MODEL_NAME = 'train_all_val_all_bz_2_epoch_10_inject_init'\n",
        "IMG_DIR_ROOT = './data/rgb'\n",
        "VG_DATA_PATH = './data/custom-regions-lite.h5'\n",
        "LOOK_UP_TABLES_PATH = './data/custom-regions-dicts-lite.pkl'\n",
        "MAX_TRAIN_IMAGE = -1\n",
        "MAX_VAL_IMAGE = -1\n",
        "\n",
        "\n",
        "def set_args():\n",
        "\n",
        "    args = dict()\n",
        "\n",
        "    args['backbone_pretrained'] = True\n",
        "    args['return_features'] = False\n",
        "\n",
        "    # Caption parameters\n",
        "    args['feat_size'] = 4096\n",
        "    args['hidden_size'] = 512\n",
        "    args['max_len'] = 16\n",
        "    args['emb_size'] = 512\n",
        "    args['rnn_num_layers'] = 1\n",
        "    args['vocab_size'] = 10629\n",
        "    args['fusion_type'] = 'init_inject'\n",
        "\n",
        "    # Training Settings\n",
        "    args['detect_loss_weight'] = 1.\n",
        "    args['caption_loss_weight'] = 1.\n",
        "    args['lr'] = 1e-4\n",
        "    args['caption_lr'] = 1e-3\n",
        "    args['weight_decay'] = 0.\n",
        "    args['batch_size'] = 4\n",
        "    args['use_pretrain_fasterrcnn'] = True\n",
        "    args['box_detections_per_img'] = 50\n",
        "\n",
        "    if not os.path.exists(os.path.join(CONFIG_PATH, MODEL_NAME)):\n",
        "        os.mkdir(os.path.join(CONFIG_PATH, MODEL_NAME))\n",
        "    with open(os.path.join(CONFIG_PATH, MODEL_NAME, 'config.json'), 'w') as f:\n",
        "        json.dump(args, f, indent=2)\n",
        "\n",
        "    return args\n",
        "\n",
        "\n",
        "def save_model(model, optimizer, amp_, results_on_val, iter_counter, flag=None):\n",
        "\n",
        "    state = {'model': model.state_dict(),\n",
        "             'optimizer': optimizer.state_dict(),\n",
        "             'amp': amp_.state_dict(),\n",
        "             'results_on_val':results_on_val,\n",
        "             'iterations': iter_counter}\n",
        "    if isinstance(flag, str):\n",
        "        filename = os.path.join('model_params', '{}_{}.pth.tar'.format(MODEL_NAME, flag))\n",
        "    else:\n",
        "        filename = os.path.join('model_params', '{}.pth.tar'.format(MODEL_NAME))\n",
        "    print('Saving checkpoint to {}'.format(filename))\n",
        "    torch.save(state, filename)\n",
        "\n",
        "\n",
        "def train(args):\n",
        "\n",
        "    print('Model {} start training...'.format(MODEL_NAME))\n",
        "\n",
        "    model = densecap_resnet50_fpn(backbone_pretrained=args['backbone_pretrained'],\n",
        "                                  feat_size=args['feat_size'],\n",
        "                                  hidden_size=args['hidden_size'],\n",
        "                                  max_len=args['max_len'],\n",
        "                                  emb_size=args['emb_size'],\n",
        "                                  rnn_num_layers=args['rnn_num_layers'],\n",
        "                                  vocab_size=args['vocab_size'],\n",
        "                                  fusion_type=args['fusion_type'],\n",
        "                                  box_detections_per_img=args['box_detections_per_img'])\n",
        "    if args['use_pretrain_fasterrcnn']:\n",
        "        model.backbone.load_state_dict(fasterrcnn_resnet50_fpn(pretrained=True).backbone.state_dict(), strict=False)\n",
        "        model.rpn.load_state_dict(fasterrcnn_resnet50_fpn(pretrained=True).rpn.state_dict(), strict=False)\n",
        "\n",
        "    model.to(device)\n",
        "\n",
        "    optimizer = torch.optim.Adam([{'params': (para for name, para in model.named_parameters()\n",
        "                                    if para.requires_grad and 'box_describer' not in name)},\n",
        "                                  {'params': (para for para in model.roi_heads.box_describer.parameters()\n",
        "                                              if para.requires_grad), 'lr': args['caption_lr']}],\n",
        "                                  lr=args['lr'], weight_decay=args['weight_decay'])\n",
        "\n",
        "    # apex initialization\n",
        "    opt_level = 'O1'\n",
        "    model, optimizer = amp.initialize(model, optimizer, opt_level=opt_level)\n",
        "    model.roi_heads.box_roi_pool.forward = \\\n",
        "        amp.half_function(model.roi_heads.box_roi_pool.forward)\n",
        "\n",
        "    train_set = TheatreDataset(IMG_DIR_ROOT, VG_DATA_PATH, LOOK_UP_TABLES_PATH, dataset_type='train')\n",
        "    val_set = TheatreDataset(IMG_DIR_ROOT, VG_DATA_PATH, LOOK_UP_TABLES_PATH, dataset_type='val')\n",
        "    idx_to_token = train_set.look_up_tables['idx_to_token']\n",
        "\n",
        "    if MAX_TRAIN_IMAGE > 0:\n",
        "        train_set = Subset(train_set, range(MAX_TRAIN_IMAGE))\n",
        "    if MAX_VAL_IMAGE > 0:\n",
        "        val_set = Subset(val_set, range(MAX_VAL_IMAGE))\n",
        "\n",
        "    train_loader = TheatreDataLoaderPFG(train_set, batch_size=args['batch_size'], shuffle=True, num_workers=2,\n",
        "                                 pin_memory=True, collate_fn=TheatreDataset.collate_fn)\n",
        "\n",
        "    iter_counter = 0\n",
        "    best_map = 0.\n",
        "    results = -1\n",
        "\n",
        "    checkpoint = torch.load(PATH)\n",
        "    model.load_state_dict(checkpoint['model'])\n",
        "    optimizer.load_state_dict(checkpoint['optimizer'])\n",
        "    iter_counter = checkpoint['iterations']\n",
        "    results = checkpoint['results_on_val']\n",
        "\n",
        "    if USE_TB:\n",
        "        writer = SummaryWriter()\n",
        "\n",
        "    for epoch in range(MAX_EPOCHS):\n",
        "\n",
        "        for batch, (img, targets, info) in enumerate(train_loader):\n",
        "            img = [img_tensor.to(device) for img_tensor in img]\n",
        "            targets = [{k:v.to(device) for k, v in target.items()} for target in targets]\n",
        "\n",
        "            model.train()\n",
        "            losses = model(img, targets)\n",
        "\n",
        "            detect_loss =  losses['loss_objectness'] + losses['loss_rpn_box_reg'] + \\\n",
        "                           losses['loss_classifier'] + losses['loss_box_reg']\n",
        "            caption_loss = losses['loss_caption']\n",
        "            total_loss = args['detect_loss_weight'] * detect_loss + args['caption_loss_weight'] * caption_loss\n",
        "\n",
        "            if USE_TB:\n",
        "                writer.add_scalar('batch_loss/total', total_loss.item(), iter_counter)\n",
        "                writer.add_scalar('batch_loss/detect_loss', detect_loss.item(), iter_counter)\n",
        "                writer.add_scalar('batch_loss/caption_loss', caption_loss.item(), iter_counter)\n",
        "\n",
        "                writer.add_scalar('details/loss_objectness', losses['loss_objectness'].item(), iter_counter)\n",
        "                writer.add_scalar('details/loss_rpn_box_reg', losses['loss_rpn_box_reg'].item(), iter_counter)\n",
        "                writer.add_scalar('details/loss_classifier', losses['loss_classifier'].item(), iter_counter)\n",
        "                writer.add_scalar('details/loss_box_reg', losses['loss_box_reg'].item(), iter_counter)\n",
        "\n",
        "\n",
        "            if iter_counter % (len(train_set)/(args['batch_size']*16)) == 0:\n",
        "                print(\"[{}][{}]\\ntotal_loss {:.3f}\".format(epoch, batch, total_loss.item()))\n",
        "                for k, v in losses.items():\n",
        "                    print(\" <{}> {:.3f}\".format(k, v))\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            with amp.scale_loss(total_loss, optimizer) as scaled_loss:\n",
        "                scaled_loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            if iter_counter > 0 and iter_counter % 10 == 0:\n",
        "                try:\n",
        "                    results = quantity_check(model, val_set, idx_to_token, device, max_iter=-1, verbose=True)\n",
        "                    if results['map'] > best_map:\n",
        "                        best_map = results['map']\n",
        "                        save_model(model, optimizer, amp, results, iter_counter)\n",
        "\n",
        "                    if USE_TB:\n",
        "                        writer.add_scalar('metric/map', results['map'], iter_counter)\n",
        "                        writer.add_scalar('metric/det_map', results['detmap'], iter_counter)\n",
        "                        writer.add_scalar('caption/meteor', results['meteor'], iter_counter)\n",
        "                        writer.add_scalar('caption/bleu', results['bleu'], iter_counter)\n",
        "                        writer.add_scalar('caption/rouge', results['rouge'], iter_counter)\n",
        "                        writer.add_scalar('caption/cider', results['cider'], iter_counter)\n",
        "\n",
        "                except AssertionError as e:\n",
        "                    print('[INFO]: evaluation failed at epoch {}'.format(epoch))\n",
        "                    print(e)\n",
        "\n",
        "            iter_counter += 1\n",
        "\n",
        "    save_model(model, optimizer, amp, results, iter_counter, flag='end')\n",
        "\n",
        "    if USE_TB:\n",
        "        writer.close()\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "\n",
        "    args = set_args()\n",
        "    train(args)"
      ],
      "metadata": {
        "id": "ZgGQ8eWtxJCJ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}